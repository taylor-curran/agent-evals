# Prompt-Generation Pipeline

Generates evaluation prompts from GitHub issues grouped by PR. Lives in `src/generate/get_gh_data/generate_prompts_from_issues.py`.

## Flow

```
pr_issue_mapping.csv → group by pr_number → fetch issues (GraphQL)
                                              ↓
                                         build prompt
                                              ↓
                            optional OpenAI summarization
                                              ↓
                           output: CSV + Markdown files
```

## 2. Core components

| Component | Responsibility |
|-----------|----------------|
| `load_pr_issue_mapping(csv_path)` | Read and return `List[Mapping]` (Dict rows). |
| `group_by_pr(rows)` | `Dict[int, list[Row]]` keyed by `pr_number`. |
| `fetch_issue_data(issue_number)` | Use GitHub GraphQL with the env var `GITHUB_TOKEN`.<br>Return title, body, first top-level comment text (optional). |
| `build_prompt_text(issue_payloads)` | Pure function → str.<br>Concatenates/summarises issues according to **Prompt Template** below. |
| `openai_summarise(raw_prompt, model="gpt-4o-mini", temperature=0.2, …)` | Optional extra condensation. Executes via `openai_agentic_framework.md` pattern. |
| `write_outputs(pr_number, prompt, csv_writer, md_file)` | Persist to files. |
| `generate_prompt_from_issues(limit=None, model=None, csv_path=…, output_csv=…, output_md=…)` | Orchestrates all steps; respects `limit` (most-recent PRs). |

---

## 3. Prompt templates

### 3.1 Summarised (default)

Simplest headless prompt – only the distilled task statement, no meta-instructions:

```
{request_summary}
```

Where `request_summary` is the single human-readable description of what needs to be built or fixed (see synthesis notes below).

The agent executing this prompt is expected to infer the appropriate planning/implementation steps on its own.

`request_summary` is generated by synthesising all related issue texts, stripping away GitHub context and merging overlapping details.

Implementation note: we will first attempt synthesis with OpenAI; if `call_openai=False`, fallback to a naive heuristic that concatenates issue titles and extracts the most common action verbs.

---

### 3.2 Raw-issue transcript (experimental)

Headless variant that feeds the full issue text directly to the agent:

```
{issue_transcript}
```

`issue_transcript` is formed by concatenating each issue’s **title**, full **body**, and (optionally) the first top-level comment, separated by `---`.

---

## 4. Parameterisation

| Parameter | Purpose | Default |
|-----------|---------|---------|
| `mode` | Either `'summary'` or `'raw'` to choose template 3.1 vs 3.2. | `'summary'` |
| `model` | OpenAI model name used **only** when `mode='summary'` and we want the API to synthesise the request. If `None`, a simple local heuristic concatenation is used. | `None` |
| `limit` | Maximum number of most-recent PRs (by `pr_number`) to process. | `None` (all) |

*Note:* In raw mode no OpenAI call is made—prompts are created purely from fetched issue data.

---

## 5. Output specification

1. **CSV**: `generated_prompts.csv` (same dir) with header:
   ```csv
   pr_number,prompt_text
   ```
2. **Markdown**: `generated_prompts.md` audit file.  For each PR we will render:

### PR 18036 – [link](https://github.com/PrefectHQ/prefect/pull/18036)

Prompt
------
<agent-prompt here>

**Linked issues**

* [#18035](https://github.com/PrefectHQ/prefect/issues/18035)
* [#18036](…)

This provides clickable shortcuts for manual review.  Prompts are fenced with `text` code blocks to avoid Markdown formatting collisions.

---

## 6. Error / Rate-limit handling

* Catch `github.GraphQLError`; retry up to 3× with exponential backoff.
* Implement GitHub fetch helpers as **Prefect tasks** (similar to `archive/fetch_gh_data/fetch_issues.py`) with `@task(retries=4, retry_delay_seconds=exponential_backoff(backoff_factor=20))` for automatic retry/back-off.
* If an issue cannot be fetched, log and skip (prompt still generated from remaining issues).
* Respect 5 000-requests/hr limit by batching queries (GraphQL allows querying multiple issues by node IDs in one request).

---

## 7. Testing strategy (future)

* Unit-test `build_prompt_text()` with synthetic issue data.
* Smoke-test full pipeline on the first 2 PRs with `limit=2`.

---

## 8. Future enhancements

* Pull additional context (labels, linked discussions).
* More intelligent summarisation chain (e.g., map-reduce with OpenAI).
* Cache GitHub queries locally to avoid re-fetching across runs.
* CLI wrapper that parses args **only** for script usage (keeping pure function API for importers).

---

_End of plan. Implementation will start after this plan is approved._