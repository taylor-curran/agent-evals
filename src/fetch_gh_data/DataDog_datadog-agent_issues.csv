number,title,state,state_reason,created_at,updated_at,closed_at,comments,labels,user,assignee,author_association,body,closed_by,reactions,timeline_url,sub_issues_total,sub_issues_completed,sub_issues_percent_completed,url
36975,Kubernetes CronJob with timeZone are detected in error with kubernetes_state.cronjob.on_schedule_check,open,,2025-05-14T14:40:52Z,2025-05-14T14:43:12Z,,0,['team/container-app'],jfaissolle,,NONE,"**Agent Environment**

agent 7.63.3

**Describe what happened:**

I setup an alert based on the `kubernetes_state.cronjob.on_schedule_check` to detect missed Cronjob schedules.

Some K8s cronjobs are setup with the Cronjob spec `timeZone` parameter.

Example:

```yaml
  schedule: 30 8 * * 1-6
  timeZone: Europe/Paris
```

With this configuration, the monitor based on this check is in error from 10:30 (UTC+2) even if the job executed at exactly 8:30 (UTC+2)

**Describe what you expected:**

No alert

**Steps to reproduce the issue:**

- Create a cronjob with a schedule based on a non UTC timeZone
- Create a monitor based on the  `kubernetes_state.cronjob.on_schedule_check` 
- Wait for job scheduled execution time

**Additional environment details (Operating System, Cloud provider, etc):**

EKS


**Analysis:**

`kube_cronjob_next_schedule_time` does not take Cronjob spec timeZone parameter into account.

https://github.com/DataDog/datadog-agent/blob/4b274f2259e53f5c0d3ac2e9d5c5867c6cee2be7/pkg/collector/corechecks/cluster/ksm/customresources/cronjob.go#L237




",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36975/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36975
36938,"[BUG] restart agent, k8s log file read from beginning",open,,2025-05-13T20:32:20Z,2025-05-13T20:59:08Z,,0,['team/agent-log-pipelines'],lerminou,,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
<!-- please include `agent version` and any other useful output (`agent status`, relevant agent logs, etc.) -->
agent 7.53

**Describe what happened:**
During a maintenance operation, all agents restart.
In Datadog, I see that numerous errors have appeared in the logs and incidents have been created.

In fact, the logs are old.
It seems that the agent, upon restart, doesn't have the latest position in each file.
the attached image show old logs  


**Describe what you expected:**
only news logs are ingested when restarting the agents

**Steps to reproduce the issue:**


**Additional environment details (Operating System, Cloud provider, etc):**
k8s on-premise 

![Image](https://github.com/user-attachments/assets/ef534b7e-15d1-419b-b95d-d7c6b6ad29e5)

![Image](https://github.com/user-attachments/assets/1dbc50ed-1180-40b6-a9a4-c90da6517654)",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36938/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36938
36921,[BUG] Metric not being tagged when process checks run in the core agent,open,,2025-05-13T14:47:46Z,2025-05-13T21:00:07Z,,2,"['team/triage', 'team/container-intake']",tsamaras,,NONE,"```
**Agent Environment**
===============
Agent (v7.65.1)
===============
  Status date: 2025-05-13 14:36:25.474 UTC (1747146985474)
  Agent start: 2025-05-13 12:54:19.295 UTC (1747140859295)
  Pid: 135078
  Go Version: go1.23.8
  Python Version: 3.12.9
  Build arch: amd64
  Agent flavor: agent
  FIPS Mode: not available
  Log File: /var/log/datadog/agent.log
  Log Level: INFO

  Paths
  =====
    Config File: /etc/datadog-agent/datadog.yaml
    conf.d: /etc/datadog-agent/conf.d
    checks.d: /etc/datadog-agent/checks.d

=============
Process Agent
=============

  Status: Not running or unreachable

=================
Process Component
=================


  Enabled Checks: [process rtprocess]
  System Probe Process Module Status: Not running
  Process Language Detection Enabled: False

  =================
  Process Endpoints
  =================
    https://process.datadoghq.com - API Key ending with:
        - b5e4f

```
**Describe what happened:**
with the change in 7.65.0 to set the process checks to run in the core agent, it appears that `datadog.process.processes.host_count` is not longer being properly tagged. we see it disappear for hosts that upgraded to that version. however if we set `run_in_core_agent: false` the metric starts populating again. The different appears to be that the process_agent sends the metric via dogstatsd, which adds appropriate tags, but the core agent looks to be sending the metric directly without adding any tags first.


**Describe what you expected:**
metric continues to be visible for all hosts.

**Steps to reproduce the issue:**
upgrade from 7.64.3 or earlier to 7.65.0+ with the default configuration for `run_in_core_agent`

**Additional environment details (Operating System, Cloud provider, etc):**
Ubuntu 20.04, 24.04
GCP
",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36921/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36921
36789,[BUG] Obfuscating possible credit card under key..,closed,completed,2025-05-08T13:23:27Z,2025-05-12T06:53:22Z,2025-05-12T06:53:20Z,4,"['team/triage', 'team/processes']",radu-sancraian,,NONE,"**Agent Environment**
gcr.io/datadoghq/serverless-init:1.6.1

**Describe what happened**

I'm using Datadog as a sidecar container alongside an ingress container in Google Cloud Run. When I attempt to ingest a span attribute with the key gcn using OpenTelemetry annotations in Java (@WithSpan, via opentelemetry-instrumentation-annotations), the value is automatically obfuscated by the Datadog agent.

Here’s the log output:

2025-05-08 12:34:30 UTC | SERVERLESS_SIDECAR | DEBUG | obfuscating possible credit card under key gcn from service campaign

From my investigation, this logic appears to originate from the Go agent here:

https://github.com/DataDog/datadog-agent/blob/main/pkg/trace/agent/obfuscate.go#L40-L48

I understand that the agent tries to protect against sensitive data, such as credit card numbers. However, in my case, gcn is a domain-specific identifier  (e.g., 4306286911211) but is not a credit card number.

**Describe what you expected**

Is there a way to disable obfuscation for specific keys like gcn?

Or, is it possible to configure a whitelist of attribute keys that should not be redacted?

Thanks!",radu-sancraian,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36789/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36789
36780,[BUG] Single step instrumentation error opening agent jar,open,,2025-05-08T04:45:00Z,2025-05-09T18:02:53Z,,1,"['team/triage', 'team/injection-platform']",prakashbalaji,betterengineering,NONE,"
**Agent Environment**
gcr.io/datadoghq/agent:7.54.0

**Describe what happened:**
When the pods start with single step instrumentation (probably on a new node but not sure), we get this error about the datadog agent jar and the pod goes into crashloopbackoff. Deleting the problematic pod, we get a new pod started correctly without any error. 

<img width=""1728"" alt=""Image"" src=""https://github.com/user-attachments/assets/cbedf0af-d4f8-43fe-8f5a-1b5c9c151fac"" />

**Describe what you expected:**

Pods should start with datadog agent instrumented everytime. 

**Steps to reproduce the issue:**

I cannot be sure about this but better to test on a new node and first time pod start on a new node. 

**Additional environment details (Operating System, Cloud provider, etc):**

AWS EKS 1.29, Self managed node groups. m5.2xlarge nodes. Spring grpc, Datadog agent with single step instrumentation enabled. ",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36780/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36780
36734,[BUG] APM remote configuration not work on 7.65.0,open,,2025-05-07T10:03:36Z,2025-05-13T18:45:04Z,,5,"['team/triage', 'team/agent-processing-and-routing']",DingGGu,,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
<!-- please include `agent version` and any other useful output (`agent status`, relevant agent logs, etc.) -->

**Describe what happened:**

I setup APM with Datadog Agent, but after updating to the latest (7.65.0)  version, an error related to Remote Configuration is occurring from the application that configured APM.

Getting error on dd-trace-rb:
```
Datadog::Core::Remote::Component#initialize') remote worker error: Datadog::Core::Remote::Client::TransportError #<Datadog::Core::Transport::InternalErrorResponse:0x0000>, error_type:Datadog::Core::Remote::Transport::HTTP::Config::Response::ParseError error:could not parse key :roots: #<JSON::ParserError:""Empty input (after ) at line 1, column 1 [parse.c:1074] in 'rpc error: code = Unknown desc = empty targets meta in director local store\n"">
```

Getting error on dd-trace-java:
```
[dd-remote-config] WARN datadog.remoteconfig.ConfigurationPoller - Failed to retrieve remote configuration: unexpected response code Internal Server Error 500 rpc error: code = Unknown desc = empty targets meta in director local store
 (Will not log warnings for 5 minutes)
```


**Describe what you expected:**


**Steps to reproduce the issue:**

Upgrade to 7.65.0 from 7.61.0


**Additional environment details (Operating System, Cloud provider, etc):**
Kubernetes with Helm installation, Helm: [datadog-3.112.0](https://github.com/DataDog/helm-charts/releases/tag/datadog-3.112.0)",,12,https://api.github.com/repos/DataDog/datadog-agent/issues/36734/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36734
36592,Vulnerabilities in Agent v7.64.3,closed,completed,2025-05-01T15:40:04Z,2025-05-08T21:55:36Z,2025-05-08T21:55:35Z,7,['team/agent-security'],JideEngDev,,NONE,"Could you please help in resolving these following vulnerabilities?

- [CVE-2025-22872](https://nvd.nist.gov/vuln/detail/cve-2025-22872) - golang.org/x/net:v0.35.0
- [CVE-2025-22870](https://nvd.nist.gov/vuln/detail/cve-2025-22870) - golang.org/x/net:v0.35.0
- [CVE-2024-12797](https://nvd.nist.gov/vuln/detail/cve-2024-12797) - cryptography:43.0.1
- [CVE-2024-40635](https://nvd.nist.gov/vuln/detail/cve-2024-40635) - github.com/containerd/containerd:v1.7.25",sgnn7,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36592/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36592
36581,[BUG] DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_GRPC_ENDPOINT and DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_HTTP_ENDPOINT environment variables not working with agent version 7.64.3,open,,2025-04-30T15:59:19Z,2025-04-30T20:35:10Z,,1,"['team/triage', 'team/opentelemetry-agent']",kamranrana-hi,,NONE,"<!--
Set the `DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_GRPC_ENDPOINT` and `DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_HTTP_ENDPOINT` environment variables. When the agent started up on version `7.64.3` it was not listening on ports `4317` and `4318`. 
-->

**Agent Environment**

- Agent version `7.64.3`

**Describe what happened:**
Set the `DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_GRPC_ENDPOINT` and `DD_OTLP_CONFIG_RECEIVER_PROTOCOLS_HTTP_ENDPOINT` environment variables. When the agent started up on version `7.64.3` it was not listening on ports `4317` and `4318`. 

**Describe what you expected:**
- GRPC endpoint is listening on port `4317`
- HTTP endpoint is listening on port `4318`


**Steps to reproduce the issue:**
- Set the environment variables 
- See that the ports are not open in the docker container where the agent is running


**Additional environment details (Operating System, Cloud provider, etc):**

- Cloud provider: AWS
- Running on ECS EC2 backed Cluster
- Running on Alpine Linux
",,1,https://api.github.com/repos/DataDog/datadog-agent/issues/36581/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36581
36525,DatadogMetric's status is flagged as Invalid if the returned Series is empty,open,,2025-04-28T16:21:10Z,2025-04-28T17:55:56Z,,2,['team/container-integrations'],qlikcoe,,NONE,"https://github.com/DataDog/datadog-agent/blob/21fe278399eae9c60bb93256a4cf3e2d3fd9c6b1/pkg/util/kubernetes/autoscalers/datadogexternal.go#L190

Not sure if that's the best approach since it could be misleading in a scenario when there is no traffic and a service isn't reporting any requests count, for example.

I'm using Keda's ScaledObject to scale using DatadogMetric object, and ScaledObject is also flagged as `not ready` because of this.  ",,2,https://api.github.com/repos/DataDog/datadog-agent/issues/36525/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36525
36447,[BUG] exit code 139 on application with high volume of logs,open,,2025-04-24T19:30:06Z,2025-04-25T17:55:23Z,,1,"['team/agent-apm', 'team/triage']",jhonatasfluency,,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
<!-- please include `agent version` and any other useful output (`agent status`, relevant agent logs, etc.) -->
Version: [7.64.3](https://github.com/DataDog/datadog-agent/releases/tag/7.64.3) 
**Describe what happened:**
We receive often exit code 139 on containers running with this agent, on past version this do not happen.

Agent execution with bash script:
```
    export DD_ENV=""prod""
    exec ddtrace-run python -m ....
```
**Describe what you expected:**
The container suddenly stop with exit code 139 

**Steps to reproduce the issue:**


**Additional environment details (Operating System, Cloud provider, etc):**
Container build on AMD64 base linux base on debian, container run on AWS ECS with Fargate 1.4.

<img width=""619"" alt=""Image"" src=""https://github.com/user-attachments/assets/55ff4175-592f-48eb-ac2c-beac2c4e756e"" />
",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36447/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36447
36411,LLM Observability Spans Not Appearing When Payload > 1MB,open,,2025-04-23T15:34:31Z,2025-04-28T09:52:19Z,,1,['team/debugger'],lucasiscovici,,NONE,"**Hello,**

### Context  
It seems that when the payload sent to the LLM Observability endpoint (`llmobs-intake.datadoghq.eu`) exceeds 1MB, the corresponding LLM Obs spans are not correctly ingested. Specifically, we notice that:

- No LLM Obs spans appear in the platform.
- There are no links between APM spans and LLM Obs spans.

In the codebase:
- The `dd-trace` library sets the max payload size as `EVP_PAYLOAD_SIZE_LIMIT = 5 << 20` (i.e. 5MB, with an actual limit of 5.1MB).
- The Datadog Agent configures the EVP proxy with `MaxPayloadSize: 5 * 1024 * 1024`.

### Question  
Does the LLM Observability intake endpoint (`llmobs-intake.datadoghq.eu`) **support and process payloads larger than 1MB**? If not, is there a recommended way to handle larger payloads for LLM Obs spans?

Thanks in advance! ",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36411/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36411
36351,log problem.,open,,2025-04-22T07:17:12Z,2025-04-22T13:17:53Z,,2,['team/agent-log-pipelines'],zdyj3170101136,,NONE,"When I use datadog agent to collect logs on k8s.

If my logs are generated too quickly, the files may be deleted before they can be collected by the agent.

Is there a way to monitor this situation?",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36351/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36351
36316,[BUG] Datadog on GKE Autopilot constantly reports ec2_prefer_imdsv2 warn messages,closed,completed,2025-04-19T16:38:03Z,2025-04-22T12:06:10Z,2025-04-22T12:05:22Z,2,['team/agent-onboarding'],Ventilateur,,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
<!-- please include `agent version` and any other useful output (`agent status`, relevant agent logs, etc.) -->

- Agent version: 7.64.1
- Helm chart version: datadog-3.110.12
- K8s node version: v1.32.2-gke.1182001
- GKE Autopilot

Values file
```
agents:
  containers:
    agent:
      resources:
        requests:
          cpu: 200m
          memory: 256Mi
        limits:
          cpu: 200m
          memory: 256Mi

    traceAgent:
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
        limits:
          cpu: 100m
          memory: 200Mi

    processAgent:
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
        limits:
          cpu: 100m
          memory: 200Mi

    systemProbe:
      resources:
        requests:
          cpu: 100m
          memory: 500Mi
        limits:
          cpu: 100m
          memory: 500Mi

  priorityClassCreate: true

clusterAgent:
  enabled: true
  metricsProvider:
    enabled: true
  rbac: 
    create: true
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 200m
      memory: 256Mi

providers:
  gke:
    autopilot: true


datadog:
  apiKeyExistingSecret: datadog-secret
  appKeyExistingSecret: datadog-secret
  site: datadoghq.eu

  ignoreAutoConfig:
    - cilium

  apm:
    socketEnabled: false
    portEnabled: true
    instrumentation:
      enabled: true
  
  logLevel: WARN
  
  logs:
    enabled: true
    containerCollectAll: true

  containerExclude: ""kube_namespace:^kube-.* kube_namespace:^gke-.*""

  prometheusScrape:
    enabled: true

  # Not supported in Autopilot yet
  # https://docs.datadoghq.com/universal_service_monitoring/setup/?tab=helm#supported-versions-and-compatibility
  serviceMonitoring:
    enabled: false
```

**Describe what happened:**

I deployed Datadog agent using Helm chart on my GKE autopilot cluster. The agent pods are flooded with these warning logs:

```
WARN | (pkg/util/ec2/imds_helpers.go:103 in doHTTPRequest) | ec2_prefer_imdsv2 is set to true in the configuration but the agent was unable to proceed: status code 404 trying to PUT http://169.254.169.254/latest/api/token
```

**Describe what you expected:**

It looks like to me that `ec2_prefer_imdsv2` is an AWS specific config, I don't expect it to happen on GCP.

**Steps to reproduce the issue:**

- Create a GKE Autopilot cluster.
- Deploy DD agent with the values file above:

```
helm install <RELEASE_NAME> \
    --set datadog.appKey=<DATADOG_APP_KEY> \
    --set datadog.site=<DATADOG_SITE> \
    -f values.yaml
    datadog/datadog
```
",tbavelier,1,https://api.github.com/repos/DataDog/datadog-agent/issues/36316/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36316
36238,[BUG] Datadog custom metrics for HPA configuration,open,,2025-04-17T12:33:56Z,2025-04-17T12:35:39Z,,0,"['team/container-app', 'team/triage']",jetndra,,NONE,As of now i dont see any option to use any datadog autogen method for HPA when we want to use some arithmetic query.. If such features are available please help us to share some link.,,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36238/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36238
36148,[BUG] Aurora Autodiscovery extra_dbname doesn't fallback to default name for engine,closed,completed,2025-04-15T10:13:09Z,2025-05-01T15:45:48Z,2025-05-01T15:45:47Z,1,"['team/database-monitoring', 'team/triage']",wparr-circle,,CONTRIBUTOR,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
<!-- please include `agent version` and any other useful output (`agent status`, relevant agent logs, etc.) -->
Chart 3.90.0, Agent 7.62.0

**Describe what happened:**
Using the new `%%extra_dbname%%` such as:
```yaml
        ad_identifiers:
          - _dbm_postgres_aurora
        cluster_check: true
        init_config:
        instances:
         - host: ""%%host%%""
           port: ""%%port%%""
           dbname: ""%%extra_dbname%%""
           tags:
            - ""dbclusteridentifier:%%extra_dbclusteridentifier%%""
            - ""region:%%extra_region%%""
```

We find that the rendered database configuration where the default dbname is used in AWS, such as ""postgres"" in the case of PostgresSQL results that this results in an invalid configuration such as:

```yaml
        instances:
         - host: ""acmedb.com""
           port: ""5432""
           dbname: """"
           tags:
            - ""dbclusteridentifier:acmedb""
            - ""region:us-east-1""
```

**Describe what you expected:**
That this renders to the default database name for the given engine type.

```yaml
        instances:
         - host: ""acmedb.com""
           port: ""5432""
           dbname: ""postgres""
           tags:
            - ""dbclusteridentifier:acmedb""
            - ""region:us-east-1""
```

**Steps to reproduce the issue:**
1. Provision an RDS cluster with the default db name
2. Attempt to configure an aurora autodiscovery based check with extra_dbname


**Additional environment details (Operating System, Cloud provider, etc):**

",sethsamuel,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36148/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36148
36112,[BUG] SQL Server Integration Errors on upgrade to 7.63.3,closed,completed,2025-04-14T15:28:35Z,2025-04-15T13:06:51Z,2025-04-15T13:06:50Z,1,"['team/integrations', 'team/triage']",JamesHaugheySlalom,,NONE,"SQLServer integration errors when agent is upgrade to `7.63.3`

**Agent Environment**
Agent image used: `gcr.io/datadoghq/agent:7.63.3` on ECS

```
Traceback (most recent call last):
File ""/opt/datadog-agent/embedded/lib/python3.12/site-packages/datadog_checks/base/checks/base.py"", line 1281, in run
initialization()
File ""/opt/datadog-agent/embedded/lib/python3.12/site-packages/datadog_checks/sqlserver/sqlserver.py"", line 335, in load_static_information
result = cursor.fetchone()
^^^^^^^^^^^^^^^^^
pyodbc.Error: ('HY091', '[HY091] [FreeTDS][SQL Server]Descriptor type out of range (0) (SQLColAttribute)')
```

**Describe what happened:**
Integration reports an error with the traceback specified above

**Describe what you expected:**
Integration to work without error as the same configuration works on 7.63.2

**Steps to reproduce the issue:**
Enabled the SQL Server integration on a Agent version > 7.63.2

**Additional environment details (Operating System, Cloud provider, etc):**
AWS ECS Fargate
",sethsamuel,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36112/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36112
36021,[BUG] sysprobe container fails to start,open,,2025-04-10T16:19:09Z,2025-04-29T14:48:16Z,,10,"['team/triage', 'team/usm']",djtecha,,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
<!-- please include `agent version` and any other useful output (`agent status`, relevant agent logs, etc.) -->

```
Agent 7.64.1 - Commit: 154bd424d2 - Serialization version: v5.0.144 - Go version: go1.23.6
Kernel: 6.8.0-57-generic
Host OS: ""Ubuntu 24.04.2 LTS""
k3s version v1.32.3+k3s1
```
 Running on a K3S cluster.

**Describe what happened:**

Container gets stuck in a crash loop. No longs but K8S events give the following:

```
Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: unable to setup user: resource temporarily unavailable```
```

**Describe what you expected:**

The container should start

**Steps to reproduce the issue:**

Enable Universal service monitoring
 

**Additional environment details (Operating System, Cloud provider, etc):**

Helm Chart:

```
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: datadog
  namespace: monitoring
spec:
  interval: 1m
  chart:
    spec:
      chart: datadog
      sourceRef:
        kind: HelmRepository
        name: datadog
        namespace: cluster-config
      version: ""3.110.10""
  values:
    datadog:
      clusterName: test
      logLevel: INFO
      apiKeyExistingSecret: ""datadog-secret""
      site: ""us3.datadoghq.com""
      tags:
        - ""env:prod""
      serviceMonitoring:
        enabled: true
      clusterChecks:
        enabled: true
      logs:
        enabled: true
        containerCollectAll: true
      processAgent:
        enabled: false
      apm:
        instrumentation:
          enabled: false
          targets:
            - name: ""default-target""
              namespaceSelector:
                matchNames:
                  - ""media""
              ddTraceVersions:
                python: ""3""
      networkMonitoring:
        enabled: false
      helmCheck:
        enabled: false
        collectEvents: true
      prometheusScrape:
        enabled: false
        serviceEndpoints: true
      envFrom:
         - secretRef:
             name: datadog-secret
      confd:
        istio.yaml: |-
          init_config:
          instances:
            - use_openmetrics: true
              send_histograms_buckets: false
              istiod_endpoint: http://istiod.istio-system.svc.cluster.local:15014/metrics
              tag_by_endpoint: false
    agents:
      podAnnotations:
        sidecar.istio.io/inject: ""false""
      volumeMounts:
        - name: systemd
          mountPath: /host/run/systemd/
      volumes:
        - name: systemd
          hostPath:
            path: /run/systemd/
    clusterAgent:
      enabled: true
      podAnnotations:
        sidecar.istio.io/inject: ""false""

```",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/36021/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/36021
35933,"[BUG] IMDSv2 / AWS Fargate - Loop of warning log, after 7.64.0",closed,completed,2025-04-08T15:55:24Z,2025-04-11T08:26:19Z,2025-04-11T08:25:28Z,1,"['team/networks', 'team/triage']",fabien-github,,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
[7.64.2](https://github.com/DataDog/datadog-agent/releases/tag/7.64.2)

On ECS Fargate.

**Describe what happened:**

Get recuring warning log :
```
UTC | CORE | WARN | (pkg/util/ec2/imds_helpers.go:103 in doHTTPRequest) | ec2_prefer_imdsv2 is set to true in the configuration but the agent was unable to proceed: Put ""http://169.254.169.254/latest/api/token"": dial tcp 169.254.169.254:80: connect: invalid argument
```

**Describe what you expected:**

No warning log.

**Steps to reproduce the issue:**

Run the agent in ECS Fargate.


**Additional environment details (Operating System, Cloud provider, etc):**

```
[
  {
    ""Name"": ""/ecs/datadog/DD_API_KEY"",
    ""Value"": ""****"",
    ""Type"": ""SecureString""
  },
  {
    ""Name"": ""/ecs/datadog/DD_CLOUD_PROVIDER_METADATA"",
    ""Value"": ""aws"",
    ""Type"": ""SecureString""
  },
  {
    ""Name"": ""/ecs/datadog/DD_CONTAINER_EXCLUDE"",
    ""Value"": ""name:aws-fargate-pause"",
    ""Type"": ""SecureString""
  },
  {
    ""Name"": ""/ecs/datadog/DD_CONTAINER_EXCLUDE_METRICS"",
    ""Value"": ""name:aws-fargate-pause"",
    ""Type"": ""SecureString""
  },
  {
    ""Name"": ""/ecs/datadog/DD_EC2_PREFER_IMDSV2"",
    ""Value"": ""false"",
    ""Type"": ""SecureString""
  },
  {
    ""Name"": ""/ecs/datadog/DD_LOG_LEVEL"",
    ""Value"": ""warn"",
    ""Type"": ""SecureString""
  },
  {
    ""Name"": ""/ecs/datadog/DD_PROCESS_AGENT_ENABLED"",
    ""Value"": ""true"",
    ""Type"": ""SecureString""
  },
  {
    ""Name"": ""/ecs/datadog/DD_REMOTE_CONFIGURATION_ENABLED"",
    ""Value"": ""false"",
    ""Type"": ""SecureString""
  },
  {
    ""Name"": ""/ecs/datadog/ECS_FARGATE"",
    ""Value"": ""true"",
    ""Type"": ""SecureString""
  },
  {
    ""Name"": ""/ecs/datadog/DD_SITE"",
    ""Value"": ""datadoghq.eu"",
    ""Type"": ""SecureString""
  }
]
```

I tried to set DD_EC2_PREFER_IMDSV2 to false, without success.
",pimlu,2,https://api.github.com/repos/DataDog/datadog-agent/issues/35933/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35933
35812,404 links in docs,open,,2025-04-04T06:07:29Z,2025-04-04T06:09:04Z,,0,['team/debugger'],B-Mahdj,,NONE,"Here are 2 404 externals links in the docs directory : 

## Errors per input

### Errors in docs/dev/agent_dev_env.md

* [404] [https://github.com/DataDog/datadog-agent/blob/main/requirements.txt](https://github.com/DataDog/datadog-agent/blob/main/requirements.txt) | Network error: Not Found

### Errors in docs/dev/linters.md

* [404] [https://golangci-lint.run/usage/faq/#how-to-integrate-golangci-lint-into-large-project-with-thousands-of-issues](https://golangci-lint.run/usage/faq/#how-to-integrate-golangci-lint-into-large-project-with-thousands-of-issues) | Network error: Not Found

I will try to make PR to fix those that I can fix. ",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35812/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35812
35808,[BUG] App Armor is still a massive issue and is NOT documented anywhere,open,,2025-04-03T23:23:53Z,2025-04-04T15:12:13Z,,0,"['team/agent-apm', 'team/triage']",matthew-williams,,NONE,"K8's Yaml applied: https://raw.githubusercontent.com/DataDog/documentation/4283a8f5332730b68aa583738af900c6f2af4bb9/static/resources/yaml/datadog-agent-all-features.yaml

**Agent Environment**
`agent:7.57.2`
- Kubernetes Daemonset


**Describe what happened:**
ptrace not allowed via apparmor, no documentation available to resolve, currently the agent yaml linked does NOT resolve this.

With process-agent enabled:
```
Apr 03 21:17:51 ip-10-104-101-121 kernel: audit: type=1400 audit(1743715071.844:2977107): apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1445831 comm=""process-agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:17:51 ip-10-104-101-121 kernel: audit: type=1400 audit(1743715071.844:2977108): apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1445831 comm=""process-agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:17:51 ip-10-104-101-121 kernel: audit: type=1400 audit(1743715071.844:2977109): apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1445831 comm=""process-agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:17:51 ip-10-104-101-121 audit[1445831]: AVC apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1445831 comm=""process-agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
```

If you disable the process-agent OR if you run process agent unconfined using: 
```
spec:
  template:
    metadata:
      annotations:
        container.apparmor.security.beta.kubernetes.io/process-agent: unconfined
```
Logs:
```
Apr 03 21:21:54 ip-10-104-101-121 audit[1462231]: AVC apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1462231 comm=""security-agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:21:54 ip-10-104-101-121 kernel: audit: type=1400 audit(1743715314.942:2977163): apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1462231 comm=""security-agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:21:54 ip-10-104-101-121 audit[1462231]: AVC apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1462231 comm=""security-agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:21:54 ip-10-104-101-121 audit[1462231]: AVC apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1462231 comm=""security-agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:21:54 ip-10-104-101-121 kernel: audit: type=1400 audit(1743715314.961:2977164): apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1462231 comm=""security-agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:21:54 ip-10-104-101-121 kernel: audit: type=1400 audit(1743715314.961:2977165): apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1462231 comm=""security-agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:23:15 ip-10-104-101-121 audit[1462091]: AVC apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1462091 comm=""agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:23:15 ip-10-104-101-121 kernel: audit: type=1400 audit(1743715395.700:2977166): apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1462091 comm=""agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:23:15 ip-10-104-101-121 kernel: audit: type=1400 audit(1743715395.701:2977167): apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1462091 comm=""agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
Apr 03 21:23:15 ip-10-104-101-121 audit[1462091]: AVC apparmor=""DENIED"" operation=""ptrace"" class=""ptrace"" profile=""cri-containerd.apparmor.d"" pid=1462091 comm=""agent"" requested_mask=""read"" denied_mask=""read"" peer=""unconfined""
```
Confirmed that the process-agent is not covered by apparmor
```
~$ sudo apparmor_status|grep agent
   snap-update-ns.amazon-ssm-agent
   snap.amazon-ssm-agent.amazon-ssm-agent
   snap.amazon-ssm-agent.ssm-cli
   /opt/datadog-agent/bin/agent/agent (1462091) cri-containerd.apparmor.d
   /opt/datadog-agent/embedded/bin/security-agent (1462231) cri-containerd.apparmor.d
   /opt/datadog-agent/embedded/bin/trace-agent (1462274) cri-containerd.apparmor.d
   /app/aws-k8s-agent (2652076) cri-containerd.apparmor.d
   /snap/amazon-ssm-agent/9881/amazon-ssm-agent (811) snap.amazon-ssm-agent.amazon-ssm-agent
```

**Describe what you expected:**
I expect documentation on how to resolve this with at least an applicable app armor profile or something similar, these logs cost money.

Neither of these configurations seem to affect the actual data the agent reports to DD. So I am not sure how its getting process data, when `ptrace` is denied. If it doesn't actually use ptrace, disable the call. If it does, provide instruction on how to remove these messages.

**Steps to reproduce the issue:**
Deploy `https://raw.githubusercontent.com/DataDog/documentation/4283a8f5332730b68aa583738af900c6f2af4bb9/static/resources/yaml/datadog-agent-all-features.yaml`

Check `journalctl -xe` for all of the errors that get posted or look in DD for all the crap that it will start ingressing on the host the container is installed in.


**Additional environment details (Operating System, Cloud provider, etc):**
AWS EKS with EC2 Nodes
Ubuntu 22.04
Kubernetes 1.29
Containerd",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35808/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35808
35781,[BUG] CVE-2025-0665,closed,completed,2025-04-03T07:23:26Z,2025-05-09T11:55:55Z,2025-05-09T11:55:54Z,9,"['team/agent-security', 'team/triage']",shiftie,,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
latest tagged Docker image (`sha256:766d72655ef255954c9e738aca2023e64e9cf823fee4fb2e79ff3617f0372b03` atm)

**Describe what happened:**
critical level vulnerability detected on CURL binary in the image:
```
_File /opt/datadog-agent/embedded/bin/curl version 8.11.1 is vulnerable to CVE-2025-0665, which exists in versions >= 8.11.1, < 8.12.0.

The vulnerability was found in the [VulnCheck NVD++ Database](https://vulncheck.com/browse/cve/CVE-2025-0665) based on the CPE cpe:2.3:a:haxx:curl and the reporting CNA has assigned it severity: Critical.

The file is associated with the technology cURL.

The vulnerability can be remediated by updating cURL to 8.12.0 or higher.
```

**Describe what you expected:**
CURL patched to remove the vulnerability.
Maybe [here](https://github.com/DataDog/datadog-agent/blob/main/Dockerfiles/agent/Dockerfile#L54)?

**Steps to reproduce the issue:**
Scan latest image.

**Additional environment details (Operating System, Cloud provider, etc):**
Not applicable.",FlorentClarret,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35781/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35781
35779,[BUG] dda inv install-tools fails with Go 1.24.2,open,,2025-04-03T01:01:42Z,2025-04-15T15:39:37Z,,4,['team/agent-runtimes'],ravelox,,NONE,"Following the build instructions for datadog-agent using Go 1.24.2 in README.md, dda inv install-tools generates the following errors:

(.venv) root@vmubuntu:/usr/local/go/src/github.com/DataDog/datadog-agent# dda inv install-tools
go: error loading go.work:
../../go.work:9: unknown godebug ""tlskyber""
[1 / 3] Failed running command `go install github.com/frapposelli/wwhrd`, retrying in 10 seconds
go: error loading go.work:
../../go.work:9: unknown godebug ""tlskyber""
[2 / 3] Failed running command `go install github.com/frapposelli/wwhrd`, retrying in 100 seconds
^X^C
Aborted.


This appears to be related to https://github.com/golang/go/issues/72111

Using Go 1.23.8 works without errors.

**Agent Environment**
Not applicable

**Describe what happened:**
When running 'dda inv install-tools' in the virtual environment, the above errors are seen.

**Describe what you expected:**
No errors.

**Steps to reproduce the issue:**
1. install Go 1.24.2
2. Clone datadog-agent repository
  git clone https://github.com/DataDog/datadog-agent.git $GOPATH/src/github.com/DataDog/datadog-agent
Cloning into '/usr/local/go/src/github.com/DataDog/datadog-agent'...
remote: Enumerating objects: 572583, done.
remote: Counting objects: 100% (3558/3558), done.
remote: Compressing objects: 100% (1635/1635), done.
remote: Total 572583 (delta 2960), reused 1923 (delta 1923), pack-reused 569025 (from 4)
Receiving objects: 100% (572583/572583), 480.04 MiB | 28.57 MiB/s, done.
Resolving deltas: 100% (434703/434703), done.
Updating files: 100% (14704/14704), done.

3. Create virtual environment
root@vmubuntu:~# cd /usr/local/go/src/github.com/Datadog/datadog-agent
root@vmubuntu:/usr/local/go/src/github.com/DataDog/datadog-agent# python3 -mvenv .venv
root@vmubuntu:/usr/local/go/src/github.com/DataDog/datadog-agent# source .venv/bin/activate

4. Install dda
(.venv) root@vmubuntu:/usr/local/go/src/github.com/DataDog/datadog-agent# pip install dda
 
5. Add Go to $PATH
(.venv) root@vmubuntu:/usr/local/go/src/github.com/DataDog/datadog-agent# export PATH=$PATH:/usr/local/go/bin

6. Run dda inv install-tools

(.venv) root@vmubuntu:/usr/local/go/src/github.com/DataDog/datadog-agent# dda inv install-tools
go: error loading go.work:
../../go.work:9: unknown godebug ""tlskyber""
[1 / 3] Failed running command `go install github.com/frapposelli/wwhrd`, retrying in 10 seconds
go: error loading go.work:
../../go.work:9: unknown godebug ""tlskyber""
[2 / 3] Failed running command `go install github.com/frapposelli/wwhrd`, retrying in 100 seconds
^X^C
Aborted.",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35779/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35779
35778,[BUG] `azure_hostname_style` is always RFC1123 non-compliant when set as `name` for Virtual Machine Scale Sets,open,,2025-04-02T23:49:25Z,2025-04-03T15:38:39Z,,0,"['team/triage', 'team/agent-runtimes']",williamoconnorme,louis-cqrl,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
<!-- please include `agent version` and any other useful output (`agent status`, relevant agent logs, etc.) -->
- agent version 7.63.0

**Describe what happened:**
In Azure, when deploying the data dog extension to a virtual machine scale set with the `azure_hostname_style` set to either `name`, `name_and_resource_group` this will always fail and default back to the computer name due to an error in the `getHostnameWithConfig()` function of the azure cloud provider

**Describe what you expected:**
I expected the extension to allow non RFC1123 compliant values, particularly underscores which are a part of the VMSS instance name. However the extension uses an older metadata API endpoint to get the instance name and this contains spaces instead of underscores. The agent should use the later API version in addition to allowing underscores as part of the name

**Steps to reproduce the issue:**
- Set `azure_hostname_style` config value to `""name""`
- Browse log output for `ValidHostname()` function output which throws an error for non RFC1123 compliant name

**Additional environment details (Operating System, Cloud provider, etc):**
- Windows Server 2022
- The name will always be RFC1123 non compliant due to the presence of an underscore in the name
- The data dog extension uses a different Azure metadata API version to that which is used in the [test case](https://github.com/DataDog/datadog-agent/blob/3ef8f5194dadfa58bb5e0bb8128bdf1571700bf3/test/new-e2e/tests/agent-subcommands/hostname/hostname_azure_nix_test.go#L46). In the test case version 2021-02-01 is used and in the [agent code](https://github.com/DataDog/datadog-agent/blob/3ef8f5194dadfa58bb5e0bb8128bdf1571700bf3/pkg/util/cloudproviders/azure/azure.go#L126C17-L126C66) version 2017-08-01 is used. The latter API returns an instance name that contains a space instead of an underscore between the VMSS resource name and the instance count. Both of these are non RFC1123 compliant. ",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35778/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35778
35767,[BUG] panic: sync: WaitGroup is reused before previous Wait has returned,open,,2025-04-02T16:30:54Z,2025-04-04T14:39:52Z,,0,"['team/serverless', 'team/triage', 'team/agent-log-pipelines']",darwayne,,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
- AWS Lambda ARM Environment
- Docker Lambda extension version: 68
- Go based binary
- Agent Version: 7.62.0 (based on the lambda extension version)

**Describe what happened:**
I am getting the following stack trace sporadically (note I see an open issue https://github.com/DataDog/datadog-lambda-extension/issues/564 as well about this; and left a similar comment there)

```
panic: sync: WaitGroup is reused before previous Wait has returned

goroutine 4285 [running]:
sync.(*WaitGroup).Wait(0x4000397df0)
    /root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.3.linux-arm64/src/sync/waitgroup.go:120 +0xac
github.com/DataDog/datadog-agent/pkg/logs/pipeline.(*Pipeline).Flush(0x4000386820, {0x1e77650, 0x400037d7a0})
    /tmp/dd/datadog-agent/pkg/logs/pipeline/pipeline.go:121 +0x5c
github.com/DataDog/datadog-agent/pkg/logs/pipeline.(*provider).Flush(0x1?, {0x1e77650, 0x400037d7a0})
    /tmp/dd/datadog-agent/pkg/logs/pipeline/provider.go:209 +0x48
github.com/DataDog/datadog-agent/comp/logs/agent/agentimpl.(*logAgent).Flush(0x40003a0ea0, {0x1e77650, 0x400037d7a0})
    /tmp/dd/datadog-agent/comp/logs/agent/agentimpl/serverless.go:51 +0x94
github.com/DataDog/datadog-agent/pkg/serverless/daemon.(*Daemon).flushLogs(0x4000210960, {0x1e77650, 0x400037d7a0}, 0x400368aa50)
    /tmp/dd/datadog-agent/pkg/serverless/daemon/daemon.go:258 +0x1fc

created by github.com/DataDog/datadog-agent/pkg/serverless/daemon.(*Daemon).TriggerFlush in goroutine 4277
    /tmp/dd/datadog-agent/pkg/serverless/daemon/daemon.go:310 +0x154
```


**Describe what you expected:**
I expected to NOT see a panic in the datadog agent. As this panic is causing my lambda logic to fail


**Steps to reproduce the issue:**
Since this is a race condition I am not sure how to reproduce .. however I believe the culprits are 
- [in the pipeline.go here](https://github.com/DataDog/datadog-agent/blob/d7d41c9bc9c08c3d9fd9f59519468eb5c7a52297/pkg/logs/pipeline/pipeline.go#L121)
    - this looks to be what the panic is referencing
 -  [in batch_strategy.go here](https://github.com/DataDog/datadog-agent/blob/d7d41c9bc9c08c3d9fd9f59519468eb5c7a52297/pkg/logs/sender/batch_strategy.go#L180)
    - note this is happening in new go routine [here](https://github.com/DataDog/datadog-agent/blob/d7d41c9bc9c08c3d9fd9f59519468eb5c7a52297/pkg/logs/sender/batch_strategy.go#L105)



**Additional environment details (Operating System, Cloud provider, etc):**
",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35767/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35767
35603,Remove usage of archived github.com/pkg/errors with preference for internal libraries,open,,2025-03-28T14:30:01Z,2025-03-28T14:46:05Z,,1,['team/triage'],kbutz,,NONE,"As of November 30, 2021, the errors package [github.com/pkg/errors](https://github.com/pkg/errors) is archived and unmaintained.

To wrap errors, developers can use the new ([as of go 1.13](https://go.dev/blog/go1.13-errors#wrapping-errors-with-w)) `fmt.Errorf` formatting verb `%w`.",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35603/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35603
35526,[BUG] FQDN use or customizable cluster domain for DD_AGENT_HOST,open,,2025-03-26T17:12:36Z,2025-04-10T15:38:09Z,,1,"['team/triage', 'team/dynamic-instrumentation', 'team/container-platform']",danibaeyens,,NONE,"We're debugging an excess of DNS requests to CoreDNS. We see datadog looking for weird search domains:
```
[INFO] 10.8.29.133:36496 - 58940 ""A IN datadog.datadog.svc.cluster.local.my-namespace.svc.cluster.local. udp 84 false 512"" NXDOMAIN qr,aa,rd 177 0.000054548s
[INFO] 10.8.29.133:54141 - 35076 ""A IN datadog.datadog.svc.cluster.local.svc.cluster.local. udp 70 false 512"" NXDOMAIN qr,aa,rd 163 0.00005738s
[INFO] 10.8.29.133:51287 - 19014 ""A IN datadog.datadog.svc.cluster.local.cluster.local. udp 66 false 512"" NXDOMAIN qr,aa,rd 159 0.00007852s
[INFO] 10.8.29.133:50022 - 54612 ""A IN datadog.datadog.svc.cluster.local.eu-central-1.compute.internal. udp 82 false 512"" NXDOMAIN qr,aa,rd,ra 201 0.000037818s
[INFO] 10.8.29.133:46214 - 59056 ""A IN datadog.datadog.svc.cluster.local. udp 52 false 512"" NXDOMAIN qr,aa,rd 145 0.000046209s
```

So it makes sense, as by default in Kubernetes' resolv.conf has:
```
search my-namespace.svc.cluster.local svc.cluster.local cluster.local eu-central-1.compute.internal
nameserver 172.20.0.10
options ndots:5
```

and `datadog.datadog.svc.cluster.local` has only 4 dots, it starts looking for additional domains, duplicating the search domains. This is a 6x increase in requests for every `datadog.datadog` resolution.

Whenever a service is calling the short service name:`http://<service>` this hits on the first try of a search domain. With the next level `http://<service>.<namespace>`, it hits on the second try. I think that in case a service uses the full domain of the cluster, the recommended way would be using FQDNs (adding a `.` like `datadog.datadog.svc.cluster.local.`) to completely skip searching through the domains, and not forcing to search through all searchable domains.

Moreover, if a service decides to use a cluster domain different from `cluster.local` this:
https://github.com/DataDog/datadog-agent/blob/dc550ebe8c0d99eb21e02085aa2681b3e85cc61f/pkg/clusteragent/admission/mutate/config/mutator.go#L73-L88
will fail, as it hard-codes the domain.

What are your thoughts on using FQDNs or customizing the cluster domain? That way, I cannot only use `example.org` domain, if I need to, but also use `example.org.` or `cluster.local.` to set FQDNs, and reduce the amount of DNS calls.

**Agent Environment**
Agent (v7.62.0)

**Describe what happened:**
Agent creates an admission controller with a full domain but without FQDN

**Describe what you expected:**
I expected a customizable MutationWebhookConfiguration object.

**Steps to reproduce the issue:**
* Install datadog agent
* Inject datadog configuration into a pod
* DD_AGENT_HOST injected url is datadog.datadog.svc.cluster.local, as defined [here](https://github.com/DataDog/datadog-agent/blob/dc550ebe8c0d99eb21e02085aa2681b3e85cc61f/pkg/clusteragent/admission/mutate/config/mutator.go#L87)

**Additional environment details (Operating System, Cloud provider, etc):**
",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35526/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35526
35521,[BUG] agent crashes on startup,open,,2025-03-26T15:33:53Z,2025-03-27T09:58:21Z,,1,"['team/triage', 'team/container-integrations']",josh-coderpad,,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
 7.63.3

**Describe what happened:**
Agent crashes in a loop and stops

```
2025-03-20 13:10:14 UTC | CORE | INFO | (comp/core/tagger/collectors/workloadmeta_main.go:154 in stream) | workloadmeta tagger collector started
panic: runtime error: index out of range [2] with length 2
goroutine 355 [running]:
github.com/DataDog/datadog-agent/comp/core/workloadmeta/collectors/internal/docker.layersFromDockerHistoryAndInspect({_, _, _}, {{0xc000db0cd0, 0x47}, {0x>
        /omnibus/src/datadog-agent/src/github.com/DataDog/datadog-agent/comp/core/workloadmeta/collectors/internal/docker/docker.go:684 +0x4f5
github.com/DataDog/datadog-agent/comp/core/workloadmeta/collectors/internal/docker.(*collector).getImageMetadata(0xc000a09cc0, {0x5b5f438, 0xc000aa4d20}, >
        /omnibus/src/datadog-agent/src/github.com/DataDog/datadog-agent/comp/core/workloadmeta/collectors/internal/docker/docker.go:649 +0x436
github.com/DataDog/datadog-agent/comp/core/workloadmeta/collectors/internal/docker.(*collector).generateEventsFromImageList(0xc000a09cc0, {0x5b5f438, 0xc0>
        /omnibus/src/datadog-agent/src/github.com/DataDog/datadog-agent/comp/core/workloadmeta/collectors/internal/docker/docker.go:222 +0x128
github.com/DataDog/datadog-agent/comp/core/workloadmeta/collectors/internal/docker.(*collector).Start(0xc000a09cc0, {0x5b5f438, 0xc000aa4d20}, {0x5baff60,>
        /omnibus/src/datadog-agent/src/github.com/DataDog/datadog-agent/comp/core/workloadmeta/collectors/internal/docker/docker.go:115 +0x205
github.com/DataDog/datadog-agent/comp/core/workloadmeta/impl.(*workloadmeta).startCandidates(0xc0012c5ad0, {0x5b5f438, 0xc000aa4d20})
        /omnibus/src/datadog-agent/src/github.com/DataDog/datadog-agent/comp/core/workloadmeta/impl/store.go:565 +0x13b
github.com/DataDog/datadog-agent/comp/core/workloadmeta/impl.(*workloadmeta).startCandidatesWithRetry.func1()
        /omnibus/src/datadog-agent/src/github.com/DataDog/datadog-agent/comp/core/workloadmeta/impl/store.go:552 +0xec
github.com/cenkalti/backoff.RetryNotify(0xc0015d7f58, {0x5b25840, 0xc000a03620}, 0x0)
        /pkg/mod/github.com/cenkalti/backoff@v2.2.1+incompatible/retry.go:37 +0x168
github.com/cenkalti/backoff.Retry(...)
        /pkg/mod/github.com/cenkalti/backoff@v2.2.1+incompatible/retry.go:24
github.com/DataDog/datadog-agent/comp/core/workloadmeta/impl.(*workloadmeta).startCandidatesWithRetry(0xc0012c5ad0, {0x5b5f438, 0xc000aa4d20})
        /omnibus/src/datadog-agent/src/github.com/DataDog/datadog-agent/comp/core/workloadmeta/impl/store.go:545 +0xaa
github.com/DataDog/datadog-agent/comp/core/workloadmeta/impl.(*workloadmeta).start.func3()
        /omnibus/src/datadog-agent/src/github.com/DataDog/datadog-agent/comp/core/workloadmeta/impl/store.go:95 +0x28
created by github.com/DataDog/datadog-agent/comp/core/workloadmeta/impl.(*workloadmeta).start in goroutine 352
```




**Describe what you expected:**
Agent starts without crashing

**Steps to reproduce the issue:**
in `/etc/datadog-agent/datadog.yaml`:
```
# Managed by Ansible

site: datadoghq.com

api_key: <a valid key>

ac_exclude:
- image:.*
ac_include:
- image:us-central1-docker.pkg.dev/our-project/images/an-image-prefix
apm_config:
    enabled: false
## adding this resolves the issue
## disabling conf.d/docker.d/* and conf.d/container.d/* integrations had no effect
# autoconfig_exclude_features:
# - docker
logs_enabled: false
process_config:
    container_collection: false
    process_collection: false
    process_discovery:
        enabled: false
```

**Additional environment details (Operating System, Cloud provider, etc):**
```
PRETTY_NAME=""Ubuntu 22.04.5 LTS""
NAME=""Ubuntu""
VERSION_ID=""22.04""
VERSION=""22.04.5 LTS (Jammy Jellyfish)""
```
On a GCP/GCE VM

Some things that weren't it:
- We've been on this version for a few days, with new machines even
- We haven't updated the images mentioned by `ac_include` (I thought because of the error maybe we had). Some of them do have 1 layer though
",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35521/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35521
35362,I would like to inquire if this project supports Java attach,open,,2025-03-21T14:56:19Z,2025-03-21T14:56:19Z,,0,[],shenlvcheng,,NONE,"f it supports attach, are there any limitations? Has the official team tested Java attach?""
",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35362/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35362
35170,[BUG] Limit for autoinstrumentation init container calculated wrong,open,,2025-03-17T13:55:40Z,2025-05-12T13:12:46Z,,1,['team/injection-platform'],florianmutter,,NONE,"<!--
If you have identified a specific bug in the Agent codebase, please describe it here. If the
issue requires further debugging or investigation, that will be best accomplished via support. Please please contact Datadog [support](http://docs.datadoghq.com/help/) and send them a [flare](https://docs.datadoghq.com/agent/troubleshooting/#send-a-flare) demonstrating the issue.
-->

**Agent Environment**
```
Agent 7.63.3 - Commit: eab078f2e7 - Serialization version: v5.0.141 - Go version: go1.23.5
```

**Describe what happened:**
Datadog webhook adds init containers with invalid resource spec. CPU limit can be smaller than request which is not allowed. This happens if there are multiple container in a pod where the sum all limits is smaller than the sum of all requests (e.g. because on container does not have a limit at all).


**Describe what you expected:**
Datadog Admission controller does not create invalid spec.


**Steps to reproduce the issue:**
Create a pod with two containers like this:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo
spec:
  template:
    spec:
      containers:
        - name: container1
          resources:
            limits:
              cpu: 1000m
              memory: 512Mi
            requests:
              cpu: 1000m
              memory: 512Mi
        - name: container2
          resources:
            limits:
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
```

**Additional environment details (Operating System, Cloud provider, etc):**
GKE.

The code that does this calculation wrong is probably here: https://github.com/DataDog/datadog-agent/blob/main/pkg/clusteragent/admission/mutate/autoinstrumentation/auto_instrumentation.go#L288

Seems to have been introduced with #30266
",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35170/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35170
35074,[BUG] Incompatible with latest version of `github.com/DataDog/opentelemetry-mapping-go/pkg/otlp/attributes`,open,,2025-03-13T00:13:04Z,2025-03-17T05:06:55Z,,1,['team/triage'],katiehockman,,CONTRIBUTOR,"Hey there 👋 

`github.com/DataDog/opentelemetry-mapping-go/pkg/otlp/attributes` made a breaking change in v0.25.0, as described [in the release notes](https://github.com/DataDog/opentelemetry-mapping-go/blob/main/CHANGELOG.md#v0250). The code on the datadog-agent side was fixed[^1], but the fix hasn't been released yet.

This is causing problems for us since some of our libraries use dependabot to automatically update all dependencies in the `go.mod` file to the latest _released_ version (not including RCs). i.e.

the otlp mapping code gets upgraded to a version with a breaking change:

```
$ go get -u github.com/DataDog/opentelemetry-mapping-go/pkg/otlp/attributes
go: upgraded github.com/DataDog/opentelemetry-mapping-go/pkg/otlp/attributes v0.24.0 => v0.26.0
```

but the datadog-agent code doesn't get upgraded to a version with a fix:

```
$ go get -u github.com/DataDog/datadog-agent/pkg/trace                           
go: upgraded github.com/DataDog/datadog-agent/pkg/trace v0.63.0 => v0.63.3
```

Which is causing our builds to fail:
```
../../go/pkg/mod/github.com/!data!dog/datadog-agent/pkg/trace@v0.63.3/traceutil/otel_util.go:543:46: not enough arguments in call to tr.ResourceToSource
        have (context.Context, pcommon.Resource, ""go.opentelemetry.io/otel/attribute"".Set)
        want (context.Context, pcommon.Resource, ""go.opentelemetry.io/otel/attribute"".Set, ""github.com/DataDog/opentelemetry-mapping-go/pkg/otlp/attributes"".HostFromAttributesHandler)
FAIL    github.com/<...> [build failed]
```

For now we can upgrade to the release candidate manually for a few libraries or downgrade the agent, but it's difficult to do that everywhere.

As an aside, I understand that these are both v0 releases so _technically_ don't have to follow Go compatibility promises, but it's quite disruptive to making breaking changes, even if it's fixed in other Datadog libraries. Can this be avoided as a general rule? 🙏 


[^1]: https://github.com/DataDog/datadog-agent/blob/main/pkg/trace/traceutil/otel_util.go#L543",,1,https://api.github.com/repos/DataDog/datadog-agent/issues/35074/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35074
35038,Identify and strip invalid Unicode characters,open,,2025-03-11T23:43:16Z,2025-03-18T18:06:40Z,,3,['team/agent-metric-pipelines'],kevinburkesegment,,NONE,"Our team maintains a metrics pipeline for our company. Teams send metric data to us and we process it using the datadog-agent. 

Recently, we replaced another component of our metric pipeline with the datadog agent, and shortly after this, suffered a metric data outage. It turned out that a customer team was sending metric data that included invalid Unicode code points. We sent this from the Datadog agent to a Vector instance using the /api/v1/series endpoint. The Vector instance uses serde-json in strict mode to parse incoming data: https://github.com/vectordotdev/vector/blob/master/src/sources/datadog_agent/metrics.rs#L416-L421

This led all of the metric data in the payload to get rejected, we would retry blindly, and eventually drop metrics.

I was sort of curious how this could happen since the standard library encoding/json.Marshal method replaces invalid Unicode code points with \ufffd, the Unicode replacement character. 

It turns out that the datadog agent uses github.com/json-iterator/go to encode JSON in the fast path. Despite a promise of ""100% compatibility with standard lib,"" this library does not handle invalid Unicode code points the same way as the standard library. Here is an open ticket from 2019 pointing out this behavior divergence; it's also easy to see by comparing the json-iterator method to the comparable standard library function. (I wrote some of the JSON escaping code in the Go standard library.) https://github.com/json-iterator/go/issues/323

While we can work with customer teams to ensure they are sending 100% valid Unicode/UTF-8 data in all instances, it would be easiest if we could strip out this data in the datadog-agent. Please either allow an option to do this, or upgrade the JSON encoding library you use to one that is 1) maintained 2) compatible with the standard library. ",,2,https://api.github.com/repos/DataDog/datadog-agent/issues/35038/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35038
35037,Duplicate logs for FastAPI python applications after bumping DD agent form 7.59.0 to 7.61.0,open,,2025-03-11T23:29:08Z,2025-03-12T16:48:27Z,,1,[],spolloni,,NONE,"We have a few FastAPI python micro-services deployed in k8s, all of which are instrumented with dd-trace-py using the `ddtrace-run` CLI to prefacing the unicorn server command. We use the datadog helm chart 

We use the [datadog helm charts](https://github.com/DataDog/helm-charts) to manage the datadog agents within our cluster. When bumping the helm chart from `3.87.0` to `3.88.0`, thereby upgrading the agent from 7.59.0 to 7.61.0, we notice the same logs appear twice in the APM's endpoint plots, under different paths even though they are the same calls.

7.59.0 -> 7.61.0

![Image](https://github.com/user-attachments/assets/7ca7ea65-dcb5-4f19-830e-8c9bf0f7f6b9)

7.61.0 -> 7.59.0

![Image](https://github.com/user-attachments/assets/779de381-afce-4368-bfe7-48c94922a577)",,0,https://api.github.com/repos/DataDog/datadog-agent/issues/35037/timeline,0,0,0,https://github.com/DataDog/datadog-agent/issues/35037
